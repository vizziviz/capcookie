{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/processed/X1_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('../data/processed/X1_test.csv', index_col=0)\n",
    "y_train = pd.read_csv('../data/processed/y1_train.csv', index_col=0)\n",
    "y_test = pd.read_csv('../data/processed/y1_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, plot_confusion_matrix, f1_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "cw = ['balanced', None]\n",
    "\n",
    "# svm.coef_ attribute is only availabel on linear kernels, so...\n",
    "kernel = ['linear']\n",
    "\n",
    "C = [0.01, .1, 1, 10, 100, 1000]\n",
    "gamma = ['scale','auto']\n",
    "grid = dict(class_weight=cw, kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, \n",
    "                           cv=5, scoring='f1', error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "end=timer()\n",
    "print('time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "#Best: 0.661085 using {'C': 10, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear'}\n",
    "svc= SVC(C=10, class_weight='balanced', gamma='scale', kernel='linear', probability=True, random_state=2)\n",
    "\n",
    "#fit training set\n",
    "svc.fit(X_train, y_train)\n",
    "y_predict_svc = svc.predict(X_test)\n",
    "\n",
    "#scores svc\n",
    "predprob = svc.predict_proba(X_test)[:,1]\n",
    "Accuracy_svc=svc.score(X_test,y_test)\n",
    "f1_svc = f1_score(y_test, y_predict_svc)\n",
    "recall_svc = recall_score(y_test, y_predict_svc)\n",
    "auc_svc = roc_auc_score(y_test, predprob)\n",
    "\n",
    "\n",
    "end = timer()\n",
    "print('time=', end-start)\n",
    "print('f1 score:', f1_svc)\n",
    "plot_confusion_matrix(svc, X_test, y_test)\n",
    "\n",
    "f1_original = f1_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.rc('ytick', labelsize=20) \n",
    "    plt.rc('xtick', labelsize=20)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=20)   # fontsize of the x and y labels\n",
    "    plt.title('Feature Importances for Tuned SVC Model')\n",
    "    plt.xlabel('Size of Coefficient')\n",
    "    plt.ylabel('Name of Feature')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_test.columns[svc.coef_.argsort()[0, -30:]]\n",
    "importances = np.sort(svc.coef_)[0, -30:]\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model has a relatively high F1 score and the feature importances that are readable.  Its highlighted features overlap somewhat with those of Logistic Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retuning the model with a lower C value to reduce the size of coefficients and highlight most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "#Best: 0.661085 using {'C': 10, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear'}\n",
    "svc= SVC(C=.1, class_weight='balanced', gamma='scale', kernel='linear', probability=True, random_state=2)\n",
    "\n",
    "#fit training set\n",
    "svc.fit(X_train, y_train)\n",
    "y_predict_svc = svc.predict(X_test)\n",
    "\n",
    "#scores svc\n",
    "predprob = svc.predict_proba(X_test)[:,1]\n",
    "Accuracy_svc=svc.score(X_test,y_test)\n",
    "f1_svc = f1_score(y_test, y_predict_svc)\n",
    "recall_svc = recall_score(y_test, y_predict_svc)\n",
    "auc_svc = roc_auc_score(y_test, predprob)\n",
    "\n",
    "\n",
    "end = timer()\n",
    "print('time=', end-start)\n",
    "print('f1 score:', f1_svc)\n",
    "plot_confusion_matrix(svc, X_test, y_test)\n",
    "plot_precision_recall_curve(svc, X_test, y_test)\n",
    "\n",
    "f1_score_retuned = f1_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retuned SVC Model with Lower C Value: Visualizing Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_test.columns[svc.coef_.argsort()[0, -30:]]\n",
    "importances = np.sort(svc.coef_)[0, -30:]\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This doesn't change the model significantly. The F1 Score is stil 0.65 and the top features include the same states but a slightly altered order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling to Create More from Less Represented Class (Positive Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 2)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search on resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "cw = ['balanced']\n",
    "\n",
    "# svm.coef_ attribute is only availabel on linear kernels, so...\n",
    "kernel = ['linear']\n",
    "\n",
    "C = [0.01, .1, 1, 10, 100, 1000]\n",
    "gamma = ['scale','auto']\n",
    "grid = dict(class_weight=cw, kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, \n",
    "                           cv=5, scoring='f1', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "end=timer()\n",
    "print('time:', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "parameters=[]\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    scores.append(mean)\n",
    "    parameters.append(param)\n",
    "    \n",
    "scores= np.array(scores)\n",
    "params=np.array(params)\n",
    "\n",
    "ps = params[scores.argsort()[-10:]]\n",
    "ss = scores[scores.argsort()[-10:]]\n",
    "\n",
    "for p, s in zip(ps, ss):\n",
    "    print(s, ':', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Test Data with Retuned Model, trained on Resampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start= timer()\n",
    "\n",
    "svc= SVC(C=10, class_weight='balanced', gamma='scale', kernel='linear', probability=True)\n",
    "\n",
    "#fit resampled data\n",
    "svc.fit(X_train_res, y_train_res)\n",
    "y_predict_svc_res = svc.predict(X_test)\n",
    "\n",
    "#scores svc_resampled\n",
    "predprob = svc.predict_proba(X_test)[:,1]\n",
    "Accuracy_svc_res=svc.score(X_test,y_test)\n",
    "f1_svc_res = f1_score(y_test, y_predict_svc_res)\n",
    "recall_svc_res = recall_score(y_test, y_predict_svc_res)\n",
    "auc_svc_res = roc_auc_score(y_test, predprob)\n",
    "\n",
    "end=timer()\n",
    "print('time:',end-start)\n",
    "\n",
    "print('f1 score:', f1_svc_res)\n",
    "plot_confusion_matrix(svc, X_test, y_test)\n",
    "\n",
    "f1_resampled = f1_svc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "plot_precision_recall_curve(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_test.columns[svc.coef_.argsort()[0, -30:]]\n",
    "importances = np.sort(svc.coef_)[0, -30:]\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  It is hard to read this because of so many negative coefficients. \n",
    "## Retuning the model with a lower C value to regularize the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start= timer()\n",
    "\n",
    "svc= SVC(C=.1, class_weight='balanced', gamma='scale', kernel='linear', probability=True, random_state=2)\n",
    "\n",
    "#fit resampled data\n",
    "svc.fit(X_train_res, y_train_res)\n",
    "y_predict_svc_res = svc.predict(X_test)\n",
    "\n",
    "#scores svc_resampled\n",
    "predprob = svc.predict_proba(X_test)[:,1]\n",
    "Accuracy_svc_res=svc.score(X_test,y_test)\n",
    "f1_svc_res = f1_score(y_test, y_predict_svc_res)\n",
    "recall_svc_res = recall_score(y_test, y_predict_svc_res)\n",
    "auc_svc_res = roc_auc_score(y_test, predprob)\n",
    "\n",
    "end=timer()\n",
    "print('time:',end-start)\n",
    "\n",
    "print('f1 score:', f1_svc_res)\n",
    "plot_confusion_matrix(svc, X_test, y_test)\n",
    "\n",
    "f1_resampled_retuned = f1_svc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_test.columns[svc.coef_.argsort()[0, -30:]]\n",
    "importances = np.sort(svc.coef_)[0, -30:]\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The F1 Score for this regularized model was higher and the feature importances are easier to read. It also coincides with the features highlight by the Logistic Regression model. There is some overlap with the important features from the first SVC model that I made without the resampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuned model:', f1_original)\n",
    "print('Retuned with low C value:', f1_retuned)\n",
    "print('Post-resampling tuned model:',f1_resampled)\n",
    "print('Post-resampling with low C value:', f1_resampled_retuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOM FOREST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the effects of hyperparameters on the model to help determine the values tested in GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "max_depths = np.arange(1, 111, 10)\n",
    "x_train = X_train\n",
    "x_test = X_test\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "    model = RandomForestClassifier(max_depth=max_depth)\n",
    "    model.fit(x_train, y_train)\n",
    "    train_pred = model.predict(x_train)\n",
    "    f1 = f1_score(y_train, train_pred)\n",
    "    train_results.append(f1)\n",
    "    y_pred = model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    test_results.append(f1)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label='Train F1')\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label='Test F1')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('max_depths')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train\n",
    "x_test = X_test\n",
    "n_estimators = [1, 20, 40, 60, 80, 100, 120, 150, 200]\n",
    "train_results = []\n",
    "test_results = []\n",
    "for n_estimator in n_estimators:\n",
    "    model = RandomForestClassifier(n_estimators=n_estimator)\n",
    "    model.fit(x_train, y_train)\n",
    "    train_pred = model.predict(x_train)\n",
    "    f1 = f1_score(y_train, train_pred)\n",
    "    train_results.append(f1)\n",
    "    y_pred = model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    test_results.append(f1)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(n_estimators, train_results, 'b', label='Train F1')\n",
    "line2, = plt.plot(n_estimators, test_results, 'r', label='Test F1')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train\n",
    "x_test = X_test\n",
    "max_features = list(range(1,100))\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_feature in max_features:\n",
    "    model = RandomForestClassifier(max_features=max_feature)\n",
    "    model.fit(x_train, y_train)\n",
    "    train_pred = model.predict(x_train)\n",
    "    f1 = f1_score(y_train, train_pred)\n",
    "    train_results.append(f1)\n",
    "    y_pred = model.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    test_results.append(f1)\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_features, train_results, 'b', label='Train')\n",
    "line2, = plt.plot(max_features, test_results, 'r', label='Test')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('max features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "model = RandomForestClassifier()\n",
    "max_depth = [5,10,20,30,40]\n",
    "criterion=['gini','entropy']\n",
    "n_estimators = [25, 50, 100, 150, 200]\n",
    "max_features= [5, 10, 20, 40, 60, 80]\n",
    "class_weight=['balanced']\n",
    "grid = dict(max_depth=max_depth, n_estimators=n_estimators, class_weight= class_weight, max_features=max_features, criterion=criterion)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=5, scoring='f1')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Top 10 GridSearch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "scores=[]\n",
    "parameters=[]\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    scores.append(mean)\n",
    "    parameters.append(param)\n",
    "    \n",
    "scores= np.array(scores)\n",
    "params=np.array(params)\n",
    "scores.argsort()[-10:]\n",
    "\n",
    "ps = params[scores.argsort()[-10:]]\n",
    "ss = scores[scores.argsort()[-10:]]\n",
    "\n",
    "for p, s in zip(ps, ss):\n",
    "    print(s, ':', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the best model parameters to model on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "#Best: 0.656406 using {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 10, 'max_features': 5, 'n_estimators': 100}\n",
    "model = RandomForestClassifier(class_weight='balanced', criterion='entropy', max_depth=10, max_features=5, n_estimators=100, random_state=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_rf = model.predict(X_test)\n",
    "f1_rf = f1_score(y_test, y_predict_rf)\n",
    "print(f1_rf)\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "\n",
    "f1rf_original = f1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "plot_precision_recall_curve(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot feature importances for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_test.columns[model.feature_importances_.argsort()[-30:]]\n",
    "importances = np.sort(model.feature_importances_)[-30:]\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is hard to read because after the first 4 features, all the rest are similar. It also contradicts some of the features highlighted using the Logistics Regression model and the SVM model. Again, 'MT' shows up here but Montana had zero Black or Hispanic people in the fatal encounters dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest GridSearch with Resampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "model = RandomForestClassifier()\n",
    "max_depth = [5,10,15,20,30]\n",
    "criterion=['gini','entropy']\n",
    "n_estimators = [50, 100, 150, 200]\n",
    "max_features= [5, 10, 15, 20, 30]\n",
    "class_weight=['balanced']\n",
    "grid = dict(max_depth=max_depth, n_estimators=n_estimators, class_weight= class_weight, max_features=max_features, criterion=criterion)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=5, scoring='f1')\n",
    "grid_result = grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 GridSearch Results for Resampled Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "scores=[]\n",
    "parameters=[]\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    scores.append(mean)\n",
    "    parameters.append(param)\n",
    "    \n",
    "scores= np.array(scores)\n",
    "params=np.array(params)\n",
    "scores.argsort()[-10:]\n",
    "\n",
    "ps = params[scores.argsort()[-10:]]\n",
    "ss = scores[scores.argsort()[-10:]]\n",
    "\n",
    "for p, s in zip(ps, ss):\n",
    "    print(s, ':', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using best model parameters on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model for test data was 3rd best GridSearch model\n",
    "model = RandomForestClassifier(class_weight='balanced', criterion='gini', \n",
    "                               max_depth=20, max_features=5, n_estimators=200, random_state=2)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "y_predict_rf = model.predict(X_test)\n",
    "f1 = (f1_score(y_test, y_predict_rf))\n",
    "print('F1:', f1)\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "plot_precision_recall_curve(model, X_test, y_test)\n",
    "\n",
    "f1rf_resampled = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Plotting feature importance for Random Forest trained on a Resampled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_test.columns[model.feature_importances_.argsort()[-30:]]\n",
    "importances = np.sort(model.feature_importances_)[-30:]\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This highlights different features but again, only the top 2 show real significance while the rest are very similar in coefficient. This also shows every single day of the week as an important feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Random Forest: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 of first tuned model:', f1rf_original)\n",
    "pritn('F1 Score, post-resampling and retuning:', f1rf_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
